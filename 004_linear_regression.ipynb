{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data\n",
    "n = 50\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(n)\n",
    "y = 2 * x - 5 + rng.randn(n)\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "ax.grid(color='#b7b7b7', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "plt.scatter(x,y, color='#333333', alpha=0.7)\n",
    "ax.axvline(6, color='#121212', linestyle='--', linewidth=1, alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predit the value of the incoming points, the simple solution is to approximate $y$ as a continuous linear function of $x$:\n",
    "\\begin{equation}\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1x_1\n",
    "\\end{equation}\n",
    "Let's start with random values of $\\theta_0$ and $\\theta_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = [np.random.randint(-10, 10), np.random.randint(-10, 10)]\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = theta[0] + theta[1] * xfit\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "ax.grid(color='#b7b7b7', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "plt.scatter(x,y, color='#333333', alpha=0.7)\n",
    "plt.plot(xfit,yfit, color='#333333')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a **cost function** to lets us figure out how to fit the best straight line to our data.\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, y, theta):\n",
    "    cost = 0\n",
    "    m = len(x)\n",
    "    for i in range(m):\n",
    "        cost += ((theta[0] + theta[1]*x[i]) - y[i])**2\n",
    "    return cost / m * 0.5\n",
    "\n",
    "theta = [np.random.randint(-10, 10), np.random.randint(-10, 10)]\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = theta[0] + theta[1] * xfit\n",
    "\n",
    "error = compute_cost(x,y,theta)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "ax.grid(color='#b7b7b7', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "ax.text(0, 10, f'error = {error:.2f}',fontsize=12,color='#000000')\n",
    "plt.scatter(x,y, color='#333333', alpha=0.7)\n",
    "plt.plot(xfit,yfit, color='#333333')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to minimize cost function to make the hypothesis as accurate as possible.\n",
    "\n",
    "Here comes **Gradient Descent**.\n",
    "\\begin{equation}\n",
    "\\theta_j := \\theta_j - \\alpha\\frac{\\delta}{\\delta\\theta_j}J(\\theta_0, \\theta_1)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "j = \\left\\{ \\begin{array}{lcl}\n",
    "0: \\frac{\\delta}{\\delta\\theta_0}J(\\theta_0, \\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)}) \\\\ \n",
    "1: \\frac{\\delta}{\\delta\\theta_1}J(\\theta_0, \\theta_1) = \\frac{1}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}\n",
    "\\end{array}\\right.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent(x, y, theta, iterations, learning_rate):\n",
    "    m = len(x)\n",
    "    theta_0, theta_1 = theta\n",
    "    costs = np.zeros(iterations)\n",
    "    for i in range(iterations):\n",
    "        tmp_0 = 0\n",
    "        tmp_1 = 0\n",
    "        for j in range(m):\n",
    "            tmp_0 += (theta_0 + theta_1*x[j]) - y[j]\n",
    "            tmp_1 += ((theta_0 + theta_1*x[j]) - y[j])*x[j]\n",
    "        theta_0 -= learning_rate / m * tmp_0\n",
    "        theta_1 -= learning_rate / m * tmp_1\n",
    "        costs[i] = compute_cost(x,y,[theta_0, theta_1])\n",
    "    return ([theta_0, theta_1], costs)\n",
    "\n",
    "iterations = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "new_theta, costs = gradient_decent(x, y, theta, iterations, learning_rate)\n",
    "yfit = new_theta[0] + new_theta[1] * xfit\n",
    "\n",
    "cost = compute_cost(x,y,new_theta)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "ax.grid(color='#b7b7b7', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "ax.text(0, 10, f'error = {cost:.2f}',fontsize=12,color='#000000')\n",
    "plt.scatter(x,y, color='#333333', alpha=0.7)\n",
    "plt.plot(xfit,yfit, color='#333333')\n",
    "ax.axvline(6, color='#121212', linestyle='--', linewidth=1, alpha=0.9)\n",
    "plt.scatter(6,yfit[600], s=100, c='#212121', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "ax.grid(color='#b7b7b7', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "plt.plot(costs, color='#333333')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's convinent and effecient to use matrix than old school iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_decent_vector(x, y, theta, iterations, learning_rate):\n",
    "    for i in range(iterations):\n",
    "        theta -= learning_rate / len(y) * np.matmul(np.matmul(theta,np.transpose(x)) -y, x)\n",
    "    return theta\n",
    "\n",
    "\n",
    "theta = [np.random.randint(-10, 10), np.random.randint(-10, 10)]\n",
    "\n",
    "x_vector = np.column_stack([np.ones([x.shape[0], 1]), x])\n",
    "\n",
    "iterations = 1000\n",
    "learning_rate = 0.01\n",
    "new_theta = gradient_decent_vector(x_vector, y, theta, iterations, learning_rate)\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = new_theta[0] + new_theta[1] * xfit\n",
    "\n",
    "cost = compute_cost(x,y,new_theta)\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "ax.grid(color='#b7b7b7', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "ax.text(0, 10, f'error = {cost:.2f}',fontsize=12,color='#000000')\n",
    "plt.scatter(x,y, color='#333333', alpha=0.7)\n",
    "plt.plot(xfit,yfit, color='#333333')\n",
    "ax.axvline(6, color='#121212', linestyle='--', linewidth=1, alpha=0.9)\n",
    "plt.scatter(6,yfit[600], s=100, c='#212121', alpha=0.7)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
