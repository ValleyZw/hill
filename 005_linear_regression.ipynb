{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to draw regression line\n",
    "def draw_result(x, y, xfit, yfit, cost=None, x_predict=None):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    ax = plt.gca()\n",
    "    ax.grid(color='#b7b7b7', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    plt.scatter(x,y, color='#333333', alpha=0.7)\n",
    "    plt.plot(xfit,yfit, color='#333333')\n",
    "    if x_predict:\n",
    "        ax.axvline(x_predict, color='#121212', linestyle='--', linewidth=1, alpha=0.9)\n",
    "        plt.scatter(x_predict,yfit[x_predict*100], s=100, c='#212121', alpha=0.7)\n",
    "    if cost:\n",
    "        ax.text(0, 10, f'error = {cost:.2f}',fontsize=12,color='#000000')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random data\n",
    "sample_size = 50\n",
    "rng = np.random.RandomState(1)\n",
    "# generate input and output data with shape = (SAMPLE, FEATURE)\n",
    "x = np.array([10 * rng.rand(sample_size)]).T\n",
    "y = 2 * x - 5 + np.array([rng.rand(sample_size)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predit the value of the incoming points, the simple solution is to approximate $y$ as a continuous linear function of $x$:\n",
    "\\begin{equation}\n",
    "\\hat{y} = f(x, \\mathbf{w}) = \\omega_0 + \\omega_1x\n",
    "\\end{equation}\n",
    "\n",
    "Objective: find $\\mathbf{w}$ which minimize the error.\n",
    "\\begin{equation}\n",
    "J(\\mathbf{w}) = \\frac{1}{2n}\\sum_{i=1}^{N}(f(x_i, \\mathbf{w}) - y_i)^2\n",
    "\\end{equation}\n",
    "\n",
    "Let's start with random values of $\\omega_0$ and $\\omega_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize weights with shape = (INPUT NODES, OUTPUT NODES)\n",
    "weights = np.array([10 * rng.rand(2)]).T\n",
    "\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = weights[0] + weights[1] * xfit\n",
    "\n",
    "draw_result(x, y, xfit, yfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Optimize fit line with linear regression.\n",
    "\n",
    "### 1. Feedforward\n",
    "\\begin{equation}\n",
    "\\mathbf{\\hat{y}} = XW\n",
    "\\end{equation}\n",
    "\n",
    "### 2. Compute cost function\n",
    "\\begin{equation}\n",
    "J(\\mathbf{y}, \\mathbf{\\hat{y}}) = \\frac{1}{2m}(\\mathbf{\\hat{y}} - \\mathbf{y})^T(\\mathbf{\\hat{y}} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n",
    "### 3. Backpropagation\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial W} \n",
    "= \\frac{\\partial J(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial \\mathbf{\\hat{y}}} \\cdot \\frac{\\partial \\mathbf{\\hat{y}}}{\\partial W} \n",
    "= \\frac{1}{m}X^T(\\mathbf{\\hat{y}} - \\mathbf{y})\n",
    "\\end{equation}\n",
    "\n",
    "### 4. Gradient descent\n",
    "\\begin{equation}\n",
    "W = W - \\alpha  \\frac{\\partial J(\\mathbf{y}, \\mathbf{\\hat{y}})}{\\partial W}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \"\"\" Simple linear regression \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _init_params(self, x, y, iterations, learning_rate, reg_factor):\n",
    "        \"\"\" Initilize parameters. \n",
    "        \n",
    "        ----------\n",
    "        W : ndarray, shape (n_features+1,)\n",
    "            Coefficient vector\n",
    "        \"\"\"\n",
    "        self._X = np.hstack([np.ones((x.shape[0], 1)), x])\n",
    "        self._y = y\n",
    "        self._learning_rate = learning_rate\n",
    "        self._reg_factor = reg_factor\n",
    "        self.weights_ = np.random.rand(self._X.shape[1],1)\n",
    "        self.costs_ = np.zeros(iterations)\n",
    "    \n",
    "    def _feedforward(self):\n",
    "        \"\"\" Computes np.dot(X, W). \"\"\"\n",
    "        self._y_hat = self._X.dot(self.weights_)\n",
    "        \n",
    "    def _backprop(self):\n",
    "        \"\"\" Update weights. \"\"\"\n",
    "        m = len(self._y)\n",
    "        # update weights with L2 regularization term\n",
    "        _weights = self.weights_.copy()\n",
    "        # ignore bias term\n",
    "        _weights[0, 0] = 0 \n",
    "        self.weights_ -= self._learning_rate * self._X.T.dot(self._y_hat - self._y)/m + self._reg_factor/m*_weights\n",
    "    \n",
    "    def _get_cost(self):\n",
    "        \"\"\" Compute loss. \"\"\"\n",
    "        m = len(self._y)\n",
    "        errors = self._y-self._y_hat\n",
    "        # cost function with L2 regularization term\n",
    "        _weights = self.weights_.copy()\n",
    "        # ignore bias term\n",
    "        _weights[0, 0] = 0    \n",
    "        return 0.5/m * errors.T.dot(errors) + self._reg_factor/(2*m)*_weights.T.dot(_weights)\n",
    "\n",
    "    def fit(self, x, y, iterations=1000, learning_rate=0.02, reg_factor=0.5):\n",
    "        \"\"\" Fit model.\n",
    "        \n",
    "        ----------\n",
    "        x : ndarray, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Target data\n",
    "        \"\"\"\n",
    "        self._init_params(x, y, iterations, learning_rate, reg_factor)\n",
    "        \n",
    "        # train model\n",
    "        for i in range(iterations):\n",
    "            self._feedforward()\n",
    "            self._backprop()\n",
    "            self.costs_[i] = self._get_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear function\n",
    "lr = LinearRegression()\n",
    "lr.fit(x, y)\n",
    "\n",
    "Xfit = np.vstack((np.ones((xfit.shape[0], )), xfit)).T\n",
    "yfit = Xfit.dot(lr.weights_)\n",
    "\n",
    "draw_result(x, y, xfit, yfit, cost=lr.costs_[-1], x_predict=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial function\n",
    "x_ = np.hstack([x, x**2])\n",
    "y_ = 2 * x + 0.6 * x**2 - 0.08 * x**3 - 5 + np.array([rng.rand(sample_size)]).T\n",
    "\n",
    "lr_ = LinearRegression()\n",
    "lr_.fit(x_, y_, iterations=50000, learning_rate=0.001, reg_factor=0.005)\n",
    "\n",
    "Xfit_ = np.vstack([np.ones((xfit.shape[0], )), xfit, xfit**2]).T\n",
    "yfit_ = Xfit_.dot(lr_.weights_)\n",
    "\n",
    "draw_result(x, y_, xfit, yfit_, cost=lr_.costs_[-1], x_predict=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
